\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{6969} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
% \title{Final Project Proposal - <Copy paste from Ravi>}
\title{Implementing Learning-Based Light Field View Synthesis}

\author{Thomas Lauer\\
UC San Diego\\
9500 Gilman Drive, San Diego CA\\
{\tt\small tlauer@ucsd.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Winston Durand\\
UC San Diego\\
9500 Gilman Drive, San Diego CA\\
{\tt\small wdurand@ucsd.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Light field photography allows for the capture of images from multiple
perspectives using a single camera array or array of microlenses. However
these come at a tradeoff between spatial resolution and angular resolution.
As the number of the microlens elements increases, the resulting lightfields
have higher angular resolution which is useful for depth estimation,
but will have lower spatial resolution for novel view synthesis.
We implement the a paper on learning based method to synthesize views from light
field camera data \cite{LearningViewSynthesis} in PyTorch.
\end{abstract}

% GUIDELINES for proposal:
% a list of four to six milestones, and deadlines to achieve the milestones
% a list of questions to be answered during the project and discussed in the report
% if the project is experimental, which existing software you will use directly or build upon
% if the project is experimental, precisely which datasets you will use and where you can obtain them quickly
% a few recent and very closely related papers that you will build on, with full bibliographic data.

%%%%%%%%% BODY TEXT
\section{Introduction}

A Lytro is a common light field camera which uses microlenses to capture multiple
perspectives of a scene in one light field. 

% Insert summary of levoy 96 (this bit is related) (needs proper citation)
Kalantari summarizes several existing methods for interpolating lightfields and their drawbacks.
Many rely on high quality input images and well defined orientations between views,
which are difficult to achieve with consumer light field cameras, and impossible with light fields
captured by hand with cellphone cameras.

Many geometric methods novel view synthesis 
such as Levoy \etal~\cite{levoy1996light} require relatively high sample counts to ensure full
coverage of the 4D lightfield, as the simple linear interpolation used by Levoy
do not work well if there are heavily occluded regions or missing data.

Wanner \etal~\cite{Wanner} utilizes an optimization based approach, which calculates disparities
using traditional computer vision as a preprocessing step. However, because the disparity estimation
is independent from the loss function, it can not be optimized as part of the training process. 

Kalantari \etal~\cite{LearningViewSynthesis} propose a method to interpolate between 
sparsely sampled sub-apertures views. They use an $8 \times 8$ subset of the full $14 \times 14$ subaperture, since the
edge pixels are often black. The four corner sub-aperture views are fed into a series of two 
networks, the first is used to predict disparity which is then used to warp the sample images to the final perspective.
The second network then takes these warped images, along with some additional metadata, and blends them together
to produce the final RGB image of the novel view. Keeping this process differentiable allows both CNNs to 
be trained at the same time, which lets the disparity estimator become tuned to work with the final CNN.

\section{Our Proposal}

Our goal is to reimplement the paper \textit{Learning-Based View Synthesis for Light Field Cameras} by 
Kalantari \etal~\cite{LearningViewSynthesis} and additionally try to apply this technique to non-Lytro camera
array captures. Further, we would like to investigate rendering novel views outside of the planar
quadrilateral formed by the source images. To evaluate the performance of our model, we will use 
the structural similarity index measure (SSIM) and peak signal to noise ratio (PSNR), which are the 
same metrics used by the original paper.

\section{Technology}

We intend to implement this using PyTorch, because it has high performance, 
automatically handles calculating derivatives, and we have previous experience with it.
Additionally, we will likely be using OpenCV or something similar for feature extraction
in the preprocessing stage.
We are not expecting to get real time performance, the paper claims it took 12.3 seconds to synthesize
a single image using a Matlab implementation. We will stitch the individual images into a video to
help visualize the results.

\section{Datasets}

Our training datasets will be Lytro captures since it represents a standardized input format which is
widely available, for example from Stanford at http://lightfields.stanford.edu/LF2016.html.
Because Lytro images are consistently formatted, we should be able to easily test our code with many different
light fields. We also want to test this approach with a camera array, and we could easily adapt our preprocessing
to accept the Stanford Multi-Camera Array. We will not be considering more free form light fields, such as those
captured by handheld phone cameras, because of the added complexity of different camera orientations.

\section{Implementation Details}


\subsection{Preprocessing}

Each lightfield is cropped to only use the inner $8 \times 8$ grid of images out of the total $14 \times 14$ available
since the microlens actually captures a circle of data. For training, we additionally ignore the outer 60 pixels where
undesirable artifacts sometimes occur and train on $60\times60$ patches with a stride of 24 pixels to ensure overlap.
Each patch has its depth data estimated for all 64 ground truths and is saved to disk. Across the 68 images from the
Stanford Lytro dataset by Raj \etal~\cite{StanfordLytro}, we synthesize roughly 600,000 unique patch views total split
90/10 between training and validation passes.

\subsection{Network}

Our network was implemented using PyTorch, using two separate convolutional networks to perform the
disparity estimation and final color assembly. Our disparity estimation network takes a 
$200 \times H \times W$ tensor as input, which we calculate using the preprocessing step.

Disparity estimation network

\begin{center}
\begin{tabular}{|c c c c|}
    Layer & Input & Output & Activation \\
    \hline
    0 & 200 & 100 & ReLU \\
    1 & 200 & 100 & ReLU \\
    2 & 100 & 50 & ReLU \\
    3 & 50 & 1 & None \\
\end{tabular}
\end{center}


This disparity is then used to warp the 4 input views:

$$
\overline{L}_{p_i} = L_{p_i} \left[ s + \left(p_i - q\right) D_q(s) \right]
$$

Where $s$ is the pixel location, $p_i$ is the location of the input view, and $q$ is the location of the
desired novel view. $D_q(s)$ is the estimated disparity at the pixel from the depth estimation network,
and $L_{p_i}(s)$ is the color of the novel view. 

Color estimation network

\begin{center}
\begin{tabular}{|c c c c|}
    Layer & Input & Output & Activation \\ 
    \hline 
    0 & 15 & 100 & ReLU \\
    1 & 200 & 100 & ReLU \\
    2 & 100 & 50 & ReLU \\
    3 & 50 & 3 & None \\
\end{tabular}
\end{center}

\section{Questions to be Answered}

How does the disparity from the first CNN compare to disparity produced 
by traditional computer vision methods, such as OpenCV?

Will the approach outlined work when we select sampled images from different locations, instead 
of the corners of the sub-aperture views? Further, is the CNN learning in this fixed grid transferrable
to other grid layouts?

Will this work if we try to reconstruct a novel view outside the bounds of the sampled sub-aperture views?

If learning in non-transferrable across capture grid aspect ratios, is it possible to solve this with
image stretching?

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
