\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{6969} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
% \title{Final Project Proposal - <Copy paste from Ravi>}
\title{Final Project Proposal - Expanding Learning-Based Light Field View Synthesis}

\author{Thomas Lauer\\
UC San Diego\\
9500 Gilman Drive, San Diego CA\\
{\tt\small tlauer@ucsd.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Winston Durand\\
UC San Diego\\
9500 Gilman Drive, San Diego CA\\
{\tt\small wdurand@ucsd.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Light field photography allows for the capture of images from multiple
perspectives using a single camera array or array of microlenses. However
these come at a tradeoff between spatial resolution and angular resolution.
As the size of the microlens elements increases, the resulting lightfield
will have higher angular resolution which is useful for depth estimation,
but will have lower spatial resolution for novel view synthesis.
We will be reimplementing a paper \cite{LearningViewSynthesis} which uses a
learning based method to synthesize views from light field camera data.
\end{abstract}

% GUIDELINES for proposal:
% a list of four to six milestones, and deadlines to achieve the milestones
% a list of questions to be answered during the project and discussed in the report
% if the project is experimental, which existing software you will use directly or build upon
% if the project is experimental, precisely which datasets you will use and where you can obtain them quickly
% a few recent and very closely related papers that you will build on, with full bibliographic data.

%%%%%%%%% BODY TEXT
\section{Introduction}

A Lytro is a common light field camera which uses microlenses to capture multiple
perspectives of a scene in one light field. 

% Insert summary of levoy 96 (this bit is related) (needs proper citation)
Many geometric methods such as Levoy \etal~\cite{levoy1996light} require relatively high sample counts to ensure full
coverage of the 4D lightfield, as the simple linear interpolation used by Levoy
do not work well if there are heavily occluded regions or missing data.

Kalantari summarizes several existing methods for interpolating lightfields and their drawbacks.
Many rely on high quality input images and well defined orientations between views,
which are difficult to achieve with consumer light field cameras, and impossible with light fields
captured by hand with cellphone cameras.

Wanner \etal~\cite{Wanner} utilizes an optimization based approach, which calculates disparities
using traditional computer vision as a preprocessing step. However, because the disparity estimation
is independent from the loss function, it can't be optimized as part of the loss function. 

Kalantari \etal~\cite{LearningViewSynthesis} propose a method to interpolate between 
sparsely sampled sub-apertures views. They use an $8 \times 8$ subset of the full $14 \times 14$ subaperture, since the
edge pixels are often black. The four corner sub-aperture views are fed into a series of two 
networks, the first is used to predict disparity which is then used to warp the sample images to the final perspective.
The second network then takes these warped images, along with some additional metadata, and blends them together
to produce the final RGB image of the novel view. Keeping this process differentiable allows both CNNs to 
be trained at the same time, which lets the disparity estimator become tuned to work with the final CNN.

\section{Our Proposal}

Our goal is to reimplement the paper \textit{Learning-Based View Synthesis for Light Field Cameras} by 
Kalantari \etal~\cite{LearningViewSynthesis} and additionally try to apply this technique to non-Lytro camera
array captures. Further, we would like to investigate rendering novel views outside of the planar
quadrilateral formed by the source images.

\section{Milestones}

Below are our milestones.

\subsection{Load Lightfields Into CNN}

Our first goal will be to load datasets into PyTorch, and set up our preprocessing pipeline.
This will involve creating dataloaders to convert the interlaced Lytro image into a stack of individual images,
extract training batches, and set up our boilerplate training loop.

\subsection{Naive CNN Approach}

After initial preprocessing, we aim to implement a single layer CNN similar to the
Naive approach outlined in the paper. This is a single convolutional network which
accepts the 4 sampled images along with their position information as input, and
attempts to produce the novel view requested. As Kalantari \etal show, this should be able
to create the novel view, but the result should be blurry and low detail. This is a good
progress check to ensure we can actually train the network correctly.

We estimate that up to this point should take approximately a week.

\subsection{Two-Part CNN With Disparity Estimation}

The next step will be implementing the two-stage CNN architecture outlined by Kalantari \etal.
Instead of feeding the raw images from the sampled views, we apply a disparity warp to all the
images at a set number of disparity levels. These are accumulated into a feature vector and used as
input to the disparity prediction network. More details can be found in section 3.1 of 
Kalantari \etal~\cite{LearningViewSynthesis}.

\subsection{Investigate Using Different Sampled Orientations}

An additional approach we would like to consider to get more spread out of the Lytor capture data
is to choose our input images in a diamond pattern rather than the $8 \times 8$ grid from the original
paper.

\subsection{Investigate Using Camera Array Datasets}

As a followup, we intend to investigate how model trained in this manner is
able to generalize to other camera array datasets, both where the views don't form
a regular square and camera orientation varies.

\subsection{Exploration of novel view outside capture bounds}

Finally, we would like to experiment with changing the virtual camera's depth, moving it gradually
away from the capture plane. A further path to test is the ability of our approach to esimate novel
views which are still on the same plane as our reference views, but outside the bounds of the
quadrilateral which they form. We could also test different numbers of sample images, the original
paper used four to cover the entire $8 \times 8$ grid, but we could investigate how more or fewer images
influence the quality of the results. 

We generally are classifying this milestone as ambition.

\section{Technology}

We intend to implement this using PyTorch, because it has high performance, 
automatically handles calculating derivatives, and we have previous experience with it.
Additionally, we will likely be using OpenCV or something similar for feature extraction
in the preprocessing stage.
We are not expecting to get real time performance, the paper claims it took 12.3 seconds to synthesize
a single image using a Matlab implementation. We will stitch the individual images into a video to
help visualize the results.

\section{Datasets}

Our training datasets will be Lytro captures since it represents a standardized input format which is
widely available, for example from Stanford at http://lightfields.stanford.edu/LF2016.html.
Because Lytro images are consistently formatted, we should be able to easily test our code with many different
light fields. We also want to test this approach with a camera array, and we could easily adapt our preprocessing
to accept the Stanford Multi-Camera Array. We won't be considering more free form light fields, such as those
captured by handheld phone cameras, because of the added complexity of different camera orientations.

\section{Questions to be Answered}

How does the disparity from the first CNN compare to disparity produced 
by traditional computer vision methods, such as OpenCV?

Will the approach outlined work when we select sampled images from different locations, instead 
of the corners of the sub-aperture views? Further, is the CNN learning in this fixed grid transferrable
to other grid layouts?

Will this work if we try to reconstruct a novel view outside the bounds of the sampled sub-aperture views?

If learning in non-transferrable across capture grid aspect ratios, is it possible to solve this with
image stretching?

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
